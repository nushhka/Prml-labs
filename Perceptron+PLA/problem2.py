# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16mBDhV7lkIPDIMrk-g_GZOvYNxwUw9FK

QUESTION 1
"""

from sklearn.preprocessing import StandardScaler
import numpy as np
import matplotlib.pyplot as plt
from time import time
from sklearn.datasets import fetch_lfw_people
from sklearn.model_selection import train_test_split
from sklearn.decomposition import PCA
from sklearn.metrics import accuracy_score
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import classification_report, ConfusionMatrixDisplay

#TASK 1

# Load the LFW dataset
lfw_people = fetch_lfw_people(min_faces_per_person=70, resize=0.4)

# Extract the data and the labels
X = lfw_people.data
y = lfw_people.target
target_names = lfw_people.target_names
n_classes = target_names.shape[0]
n_samples, h, w = lfw_people.images.shape
n_features = X.shape[1]

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

print("Total dataset size:")
print("n_samples: %d" % n_samples)
print("n_features: %d" % n_features)
print("n_classes: %d" % n_classes)

#TASK 2(VISUALISATION OF WHY N = 130?)

# Define a range of n_components values to try
n_components_range = range(10, 200, 20)

# Calculate the number of rows needed
num_rows = len(n_components_range) // 4 + (1 if len(n_components_range) % 4 != 0 else 0)

# Create subplots with num_rows rows and 4 columns
fig, axs = plt.subplots(num_rows, 4, figsize=(20, 5*num_rows))

# Flatten axs if num_rows is 1
if num_rows == 1:
    axs = axs.reshape(1, -1)

# Iterate over each n_components value and plot cumulative explained variance ratio
for i, n_components in enumerate(n_components_range):
    # Fit PCA
    pca = PCA(n_components=n_components, whiten=True, random_state=42)
    pca.fit(X_train)

    # Plot cumulative explained variance ratio on the i-th subplot
    row_index = i // 4
    col_index = i % 4
    axs[row_index, col_index].plot(pca.explained_variance_ratio_)
    axs[row_index, col_index].set_xlabel('Number of Components')
    axs[row_index, col_index].set_ylabel('Explained Variance Ratio')
    axs[row_index, col_index].set_title(f'n_components={n_components}')
    axs[row_index, col_index].grid(True)

# Remove empty subplots
if len(n_components_range) % 4 != 0:
    for j in range(len(n_components_range) % 4, 4):
        fig.delaxes(axs[num_rows - 1, j])

plt.tight_layout()
plt.show()


# REASON -
# HERE THE PLOT BECOMES STRAIGHT AT AROUNd N_COMPONENTS=130 plot,
# SO TAKING MORE COMPONENTS WOULD NOT ADD ANY VALUE TO THE VARIANCE RATIO,
# BUT ONLY INCREASE THE COMPUTATION TIME, SO HIGHER VALUE WOULDNOT BE A GOOD IDEA
# HENCE A VALUE OF 130 WOULD BE A GOOD CHOICE

#TASK 2

# Compute the PCA (eigenfaces) on the face dataset
n_components = 130
pca = PCA(n_components=n_components, whiten=True, random_state=42)
pca.fit(X_train)

# Apply PCA transformation to the training and testing data
X_train_pca = pca.transform(X_train)
X_test_pca = pca.transform(X_test)
print("Extracted the top %d eigenfaces from %d faces" % (n_components, X_train.shape[0]))

#TASK 3

knn_classifier = KNeighborsClassifier(n_neighbors=5)
# Train the classifier using the transformed training data
knn_classifier.fit(X_train_pca, y_train)

#TO CHECK WHICH IS BETTER
from sklearn.svm import SVC

# Set n_components
n_components = 130

# Compute PCA with n_components
pca = PCA(n_components=n_components, whiten=True, random_state=42)
X_train_pca = pca.fit_transform(X_train)
X_test_pca = pca.transform(X_test)

# Initialize and train KNN classifier
knn_classifier = KNeighborsClassifier(n_neighbors=5)
knn_classifier.fit(X_train_pca, y_train)

# Make predictions with KNN
knn_y_pred = knn_classifier.predict(X_test_pca)

# Calculate accuracy with KNN
knn_accuracy = accuracy_score(y_test, knn_y_pred)

# Initialize and train SVM classifier
svm_classifier = SVC(kernel='linear')
svm_classifier.fit(X_train_pca, y_train)

# Make predictions with SVM
svm_y_pred = svm_classifier.predict(X_test_pca)

# Calculate accuracy with SVM
svm_accuracy = accuracy_score(y_test, svm_y_pred)

print("Accuracy with KNN:", knn_accuracy)
print("Accuracy with SVM:", svm_accuracy)

# Compare accuracies
if knn_accuracy > svm_accuracy:
    print("KNN classifier is better.")
elif knn_accuracy < svm_accuracy:
    print("SVM classifier is better.")
else:
    print("Both classifiers perform equally well.")

#TASK 4

# Predict people's names on the test set
print("Predicting people's names on the test set")
t0 = time()
y_pred = knn_classifier.predict(X_test_pca)
print("done in %0.3fs" % (time() - t0))

# Print classification report and accuracy
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
print(classification_report(y_test, y_pred, target_names=target_names))

# Helper function to plot a gallery of portraits
def plot_gallery(images, titles, h, w, n_row=3, n_col=4):
    """Helper function to plot a gallery of portraits"""
    plt.figure(figsize=(1.8 * n_col, 2.4 * n_row))
    plt.subplots_adjust(bottom=0, left=0.01, right=0.99, top=0.90, hspace=0.35)
    for i in range(n_row * n_col):
        plt.subplot(n_row, n_col, i + 1)
        plt.imshow(images[i].reshape((h, w)), cmap=plt.cm.gray)
        plt.title(titles[i], size=12)
        plt.xticks(())
        plt.yticks(())

# Helper function to generate titles for images
def title(y_pred, y_test, target_names, i):
    pred_name = target_names[y_pred[i]].rsplit(" ", 1)[-1]
    true_name = target_names[y_test[i]].rsplit(" ", 1)[-1]
    return "predicted: %s\ntrue:      %s" % (pred_name, true_name)

# Generate titles for predicted and true labels
prediction_titles = [title(y_pred, y_test, target_names, i) for i in range(y_pred.shape[0])]

# Plot a gallery of portraits
print("VISUALISATION OF PREDICTED AND ACTUAL FACES:")
plot_gallery(X_test, prediction_titles, lfw_people.images.shape[1], lfw_people.images.shape[2])

plt.show()

print('Gallery of the most significative eigenfaces')
eigenfaces = pca.components_.reshape((n_components, h, w))
eigenface_titles = ["eigenface %d" % i for i in range(eigenfaces.shape[0])]
plot_gallery(eigenfaces, eigenface_titles, h, w)

plt.show()

# Plot confusion matrix
disp = ConfusionMatrixDisplay.from_estimator(
    knn_classifier, X_test_pca, y_test, display_labels=target_names, xticks_rotation="vertical"
)
disp.figure_.tight_layout()
plt.show()
# Identify misclassified samples
misclassified_indices = np.where(y_pred != y_test)[0]

# Display misclassified samples along with their predicted and true labels
num_misclassified = len(misclassified_indices)
print("Number of misclassified samples:", num_misclassified)

#TASK 5

# Define the range of n_components values to try
n_components_range = range(10, 201, 10)

# Initialize an empty list to store accuracies
accuracies = []

# Iterate over each n_components value
for n_components in n_components_range:
    # Compute PCA with the current number of components
    pca = PCA(n_components=n_components, whiten=True, random_state=42)

    # Apply PCA transformation to the training and testing data
    X_train_pca = pca.fit_transform(X_train)
    X_test_pca = pca.transform(X_test)

    # Initialize and train a KNN classifier
    knn_classifier = KNeighborsClassifier(n_neighbors=5)
    knn_classifier.fit(X_train_pca, y_train)

    # Make predictions on the testing data
    y_pred = knn_classifier.predict(X_test_pca)

    # Calculate accuracy and store it
    accuracy = accuracy_score(y_test, y_pred)
    accuracies.append(accuracy)

# Plot the impact of different values of n_components on accuracy
plt.figure(figsize=(10, 6))
plt.plot(n_components_range, accuracies, marker='o')
plt.xlabel('Number of Components (n_components)')
plt.ylabel('Accuracy')
plt.title('Impact of Number of Components on Accuracy')
plt.grid(True)
plt.show()

# Find the indices of the top two maximum accuracies
top_indices = np.argsort(accuracies)[::-1][:2]

# Retrieve the corresponding values of n_components
top_n_components = [n_components_range[i] for i in top_indices]

# Retrieve the corresponding maximum accuracies
top_accuracies = [accuracies[i] for i in top_indices]

print("Top two values of n_components for maximum accuracy:", top_n_components)
print("Corresponding maximum accuracies:", top_accuracies)